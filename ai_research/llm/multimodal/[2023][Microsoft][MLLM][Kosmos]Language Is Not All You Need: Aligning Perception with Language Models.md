# [2023][Microsoft][MLLM][Kosmos]Language Is Not All You Need: Aligning Perception with Language Models 

- paper: https://arxiv.org/pdf/2302.14045.pdf
- https://github.com/microsoft/unilm
----
## Intro
- Kosmos-1 is a multimodal can perceive both language and embedding
    - Support vision tasks/ mutimodual dialogue
    - OCR-free NLP
    - language understanding, generation
![](https://i.imgur.com/9k20hPK.png)
![](https://i.imgur.com/64D0W2F.png)


- Why do we need mutimodal LLM?
    - In addition to text info, realworld interaction also includes video and sound.

- Key takeaways
    - Acquire info beyond text. Step to robotics, vision understanding
    - LLM as general purpose interfaces/ universal task layer

## KOSMOS-1

- Generate text in an auto-regressive manner
- A transformer based causal model
- Other modalities(image) are embedded and fed into the language model
- Input representation
    - Flatten input as a sequence decorated with special tokens 
        - `<image> </image>` indicate the beginning and end of encoded image embeddings.
        - `<s>document</s>` indicate a text input
![](https://i.imgur.com/2lfpvOQ.png)

    - Use embedding module to encode both text tokens and other input to vectors. And then feed the embeddings to decoder.
    - Empoly a vision encoder as the embedding module for the input images
    - Use `Resampler` ad an attentive pooling mechanism to reduce the number of image embeddings

- Structures
    - [MAGNETO](https://arxiv.org/pdf/2210.06423.pdf): Microsoft's foundation transformer for general pupose modeling(text/image/audio)
        - 24 layers
        - 2048 hidden dimensions
        - 8192 FFN intermediate sizes
        - 32 attention heads
        - 1.3B parameters
    - XPOS: better generalize to different lengths and also efficient in both interpolation and extrapolation settings.

- Training Objective 
    - training data
        - monomodal data: text data
        - interleaved multimodal data: interleaved images and text
        - cross modal paired data: image caption pairs
    - Trained with the `next-token` prediciton task
    - Objective is maximize the log-likelihood

- Training Data
    - Text data
        - The Pile: a massive English text dataset 
        - Common Crawl snapshot(202005~202104)
        - RealNews datasets

    - Image caption pairs
        - LAION-2B
        - LAION-400M
        - COYO-600M
        - Conceptual Captions

    - Interleaved Image-Text Data
        - Common Crawl Snapshot

- Training Setup
    - Image representation
        - From pretrained `CLIP ViT/14` with 1024 feature dimensions
    - Preprocess image to 224x224
    - A batch size with 1.2 million tokens
        - 0.5 millions from text
        - 0.5 millions from image-caption pairs
        - 0.2 million from interleaved data
    - Train 300k steps, about 360 billion tokens
    - Use `AdamW` optimizer
    - Use Google's [SentencePiece](https://github.com/google/sentencepiece) to tokenize the text

- Language only instruction tuning
    - Combine `Unnatural Instructions` and `FLANv2` as instruction dataset
        - [2022][Meta][Unnatural Instructions](https://github.com/orhonovich/unnatural-instructions): A dataset of instructions automatically generated by a Large Language model
            - paper: https://arxiv.org/pdf/2212.09689.pdf

        - [FLANv2](https://huggingface.co/datasets/philschmid/flanv2)
